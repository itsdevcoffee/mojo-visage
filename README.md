# Mojo ML From Scratch üî•

> **Build Machine Learning from fundamentals to LLMs in Mojo**
> Learn by implementing everything yourself: Linear Algebra ‚Üí Neural Networks ‚Üí Transformers ‚Üí Language Models

[![Mojo](https://img.shields.io/badge/Mojo-0.26.1-orange?logo=fire)](https://docs.modular.com/mojo/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Progress](https://img.shields.io/badge/progress-2%2F40%20projects-yellow)](src/block0/README.md)

---

## üéØ What is This?

This is a **40+ project learning journey** where I build ML/AI systems from scratch in Mojo‚Äîno PyTorch, no TensorFlow, just pure understanding.

**The Philosophy:** You don't truly understand neural networks until you've implemented backpropagation by hand. You don't understand transformers until you've built attention from scratch. This repo is about **deep understanding**, not just using libraries.

**Why Mojo?** üî•
- Python-like syntax (easy to read)
- C-like performance (fast to run)
- Built for AI/ML from the ground up
- Perfect for learning how things *really* work

---

## üìö Learning Path

```
Block 0: Math Fundamentals        [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 18%
Block 1: Optimization              [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 2: Classification            [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 3: Backpropagation           [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 4: Modern Training           [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 5: Embeddings                [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 6: Language Modeling         [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 7: RNNs                      [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 8: Attention                 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 9: Transformers              [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 10: LLM Reality              [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
Block 11: Chat + Tool Use          [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  0%
```

**Full learning outline:** [docs/learning-outline.md](docs/learning-outline.md)

---

## üöÄ Quick Start

### Prerequisites

- Linux or macOS (Mojo not available on Windows yet)
- [Pixi](https://pixi.sh/) package manager

### Installation

```bash
# Clone the repo
git clone https://github.com/yourusername/mojo-ml-from-scratch.git
cd mojo-ml-from-scratch

# Install dependencies (Mojo, Python, NumPy, matplotlib)
pixi install

# Run your first Mojo ML code!
pixi run vector-add
pixi run dot-product

# Run tests
pixi run test-all
```

**New to Mojo?** Check out the [Mojo Cheatsheet](docs/cheatsheets/mojo-cheatsheet.md) for quick syntax reference!

---

## üí° What You'll Build

### Block 0: Core Math Tools (In Progress)
- ‚úÖ **Vector addition** - Element-wise operations and shape validation
- ‚úÖ **Dot product** - The foundation of neural networks
- üöß Matrix-vector multiply
- üöß Matrix-matrix multiply
- üîú Random number generation with seeds
- üîú Dataset splitting and batching
- üîú Plotting and visualization

[View Block 0 Progress ‚Üí](src/block0/README.md)

### Block 1: Optimization + Regression (Coming Next)
- Linear regression (closed-form solution)
- Gradient descent from scratch
- Loss functions (MSE, MAE)
- Feature scaling and regularization

### Blocks 2-11: The Journey to LLMs
- **Block 2:** Logistic regression, softmax, cross-entropy
- **Block 3:** Backpropagation without autograd (the hard way!)
- **Block 4:** Adam optimizer, dropout, batch norm
- **Block 5:** Word embeddings, similarity search
- **Block 6:** Character-level language models
- **Block 7:** RNNs and LSTMs from scratch
- **Block 8:** Attention mechanisms
- **Block 9:** Build a tiny transformer
- **Block 10:** LLM inference, fine-tuning, KV cache
- **Block 11:** Chat model with tool use

---

## üìÇ Project Structure

```
mojo-ml-from-scratch/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ block0/
‚îÇ       ‚îú‚îÄ‚îÄ 01_vector_matrix_ops/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ 01_vector_add.mojo       ‚úÖ Complete
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ 02_dot_product.mojo      ‚úÖ Complete
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ       ‚îú‚îÄ‚îÄ 02_random_utils/             üöß Next
‚îÇ       ‚îú‚îÄ‚îÄ 03_dataset_utils/
‚îÇ       ‚îî‚îÄ‚îÄ 04_plotting_utils/
‚îú‚îÄ‚îÄ tests/                                ‚úÖ 7/7 passing
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ learning-outline.md              üìñ Full curriculum
‚îÇ   ‚îî‚îÄ‚îÄ cheatsheets/
‚îÇ       ‚îî‚îÄ‚îÄ mojo-cheatsheet.md           üî• Mojo syntax reference
‚îî‚îÄ‚îÄ scripts/
```

**Organization Philosophy:** Each block ‚Üí sections ‚Üí numbered projects. Follow the numbers to learn in the right order!

---

## üß™ Testing

Every implementation has comprehensive tests:

```bash
# Run all tests
pixi run test-all

# Run specific tests
pixi run test-vector-add
pixi run test-dot-product
```

**Test-Driven Learning:** Each project includes edge cases, error handling, and mathematical properties to ensure deep understanding.

---

## üìñ Key Learnings So Far

### Vector Addition
```mojo
fn vector_add(a: List[Float64], b: List[Float64]) raises -> List[Float64]:
    if len(a) != len(b):
        raise Error("Shape mismatch!")  # 80% of ML bugs!
    var result = List[Float64]()
    for i in range(len(a)):
        result.append(a[i] + b[i])
    return result^
```
**Lesson:** Always validate shapes. Shape bugs are the #1 source of ML errors.

### Dot Product
```mojo
fn dot_product(a: List[Float64], b: List[Float64]) raises -> Float64:
    var result: Float64 = 0.0
    for i in range(len(a)):
        result += a[i] * b[i]  # Multiply and accumulate
    return result
```
**Lesson:** Dot product is EVERYTHING in ML. Neural nets, attention, loss functions‚Äîall built on this.

---

## üéì Learning Resources

- **Mojo Documentation:** [docs.modular.com/mojo](https://docs.modular.com/mojo/)
- **Mojo Cheatsheet:** [Quick syntax reference](docs/cheatsheets/mojo-cheatsheet.md)
- **Learning Outline:** [Full curriculum breakdown](docs/learning-outline.md)
- **Mojo Community:** [Discord](https://discord.gg/modular)

---

## üõ†Ô∏è Development

### Running Individual Projects

```bash
# List all available commands
pixi task list

# Run specific implementations
pixi run vector-add
pixi run dot-product

# Enter Mojo REPL for experimentation
pixi run repl
```

### Project Guidelines

1. **Build from scratch** - No ML libraries, implement everything
2. **Optimize for understanding** - Readable code > clever code
3. **Test everything** - Edge cases, error cases, mathematical properties
4. **Document learnings** - README per block with key insights

---

## üó∫Ô∏è Roadmap

- [x] Set up Mojo environment with pixi
- [x] Implement vector addition with shape validation
- [x] Implement dot product (foundation of neural nets)
- [x] Build comprehensive test suite
- [ ] Complete Block 0 (matrix operations, random utils, datasets)
- [ ] Block 1: Gradient descent and linear regression
- [ ] Block 2: Classification with logistic regression
- [ ] Block 3: Manual backpropagation (the enlightenment moment!)
- [ ] Blocks 4-11: The journey to LLMs continues...

---

## ü§ù Contributing

This is primarily a **personal learning journey**, but if you're also learning ML from scratch in Mojo:

1. **Star the repo** if you find it helpful! ‚≠ê
2. **Follow along** and build your own implementations
3. **Share your learnings** - Open issues with questions or insights
4. **Suggest improvements** - PRs welcome for bug fixes or clarity

---

## üìù License

MIT License - Feel free to use this for your own learning!

---

## üîç Topics

`mojo` `machine-learning` `deep-learning` `neural-networks` `transformers` `llm` `from-scratch` `educational` `ai` `ml-fundamentals` `backpropagation` `gradient-descent` `attention-mechanism` `learn-by-building` `build-in-public`

---

## ‚≠ê Star History

If you find this helpful, consider giving it a star! It helps others discover this learning resource.

---

**Built with Mojo üî• | Learning in Public | [Follow the Journey](https://github.com/yourusername/mojo-ml-from-scratch)**
